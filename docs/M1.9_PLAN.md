# M1.9 Plan — Split-Machine Deployment via Tailscale

## Summary

M1.9 makes the two-process architecture work across machines over Tailscale. The trainer
process runs on a GPU machine, the UI process runs on a laptop, and you open the dashboard
in a browser on the laptop. Same code, same pipeline — just configured for remote access.

## Context & Motivation

Training autoencoders on CPU is slow (the M1.75 test took ~5 minutes on a MacBook). Real
work needs a GPU. The natural split is:

- **Trainer on GPU machine** (`paul-cheddar`, Tailscale `100.121.59.123`) — heavy compute, owns model state
- **UI on laptop** (`pauls-macbook-pro`, Tailscale `100.81.176.109`) — hot-reloading dashboard, lightweight

The two-process architecture was designed for exactly this. The UI is a stateless proxy to
the trainer API. It already reads `ACC_TRAINER_URL` from an env var. The hard part is not
the code — it's making the deployment easy to launch and verifiable.

We also need the flexibility to run BOTH processes on the GPU machine and just access the
UI from the laptop's browser. This is simpler (no split) and useful for quick sessions.

## Network Topology

### Scenario A: Split (laptop UI + remote trainer)
```
  Laptop (pauls-macbook-pro / 100.81.176.109)
  ┌──────────────────────────────────────┐
  │  Browser ──> UI Process (:8080)      │
  │              │                        │
  │              │ ACC_TRAINER_URL =      │
  │              │ http://paul-cheddar:8787│
  └──────────────┼────────────────────────┘
                 │ Tailscale
  ┌──────────────┼────────────────────────┐
  │  Trainer Process (:8787)             │
  │  GPU machine (paul-cheddar / 100.121.59.123) │
  └──────────────────────────────────────┘
```

### Scenario B: All-remote (both on GPU, browser on laptop)
```
  Laptop (browser only)
  │
  │  http://paul-cheddar:8080
  │
  GPU machine (paul-cheddar / 100.121.59.123)
  ┌──────────────────────────────────────┐
  │  UI Process (:8080)                  │
  │              │                        │
  │              │ ACC_TRAINER_URL =      │
  │              │ http://localhost:8787   │
  │              │                        │
  │  Trainer Process (:8787)             │
  └──────────────────────────────────────┘
```

### Scenario C: Local dev (both on laptop)
```
  Laptop
  ┌──────────────────────────────────────┐
  │  Browser ──> UI Process (:8080)      │
  │              │                        │
  │              │ http://localhost:8787   │
  │              │                        │
  │  Trainer Process (:8787)             │
  └──────────────────────────────────────┘
```

## What's Already Done

The codebase is surprisingly close to working across machines:

1. `trainer_main.py` binds to `0.0.0.0:8787` — already listens on all interfaces
2. `ui_main.py` binds to `0.0.0.0:8080` — already listens on all interfaces
3. `ui/app.py` reads `ACC_TRAINER_URL` from env var (default `http://localhost:8787`)
4. The SSE proxy forwards from trainer to browser through the UI, so the browser never
   needs direct access to the trainer
5. All API calls go through `_api()` helper which uses `TRAINER_URL`

## What Needs to Change

### Phase 1: CLI + Config

**Outcome:** I can start the UI with `python -m acc.ui_main --trainer-url http://paul-cheddar:8787`
and the dashboard connects to a remote trainer. No env vars needed.

**Foundation:** `AccConfig` dataclass centralizes all network configuration (hosts, ports,
trainer URL). Both processes read from it. Future milestones (M2+) add more config without
scattering env vars.

**Files:**

| File | Change |
|------|--------|
| `acc/config.py` | **NEW** — `AccConfig` dataclass with `trainer_host`, `trainer_port`, `ui_host`, `ui_port`, `trainer_url` (computed). Reads from env vars with sane defaults. |
| `acc/ui_main.py` | **MODIFY** — Add `--trainer-url` CLI arg. Pass to config. Print actual trainer URL on startup. |
| `acc/trainer_main.py` | **MODIFY** — Read host/port from `AccConfig`. Print Tailscale-friendly startup message. |
| `acc/ui/app.py` | **MODIFY** — Accept `trainer_url` at app creation time instead of reading env var at module level. |

**Tasks:**

1. Create `acc/config.py` with `AccConfig` dataclass
2. Update `ui_main.py` — add `--trainer-url` arg, wire through to app
3. Update `trainer_main.py` — use config, print accessible addresses
4. Update `ui/app.py` — accept trainer_url as parameter, not global env read
5. Verify: start trainer, start UI with `--trainer-url http://localhost:8787`, dashboard loads

### Phase 2: Connectivity Health Check

**Outcome:** The dashboard shows a live connection indicator: green when the trainer is
reachable, red with an error message when it's not. I can tell immediately if my Tailscale
link is working.

**Foundation:** Health-check pattern reusable for any service-to-service connection in
future milestones.

**Files:**

| File | Change |
|------|--------|
| `acc/ui/app.py` | **MODIFY** — Add `/partial/health` endpoint that pings trainer `/health`. Show connection status in header bar. Color-coded: green=connected, red=disconnected. Show trainer URL. |

**Tasks:**

1. Add health partial endpoint that calls trainer `/health`
2. Update dashboard header to show connection status + trainer URL
3. Auto-refresh health every 3 seconds via HTMX
4. Verify: start UI without trainer running — header shows red "Disconnected". Start trainer — header turns green.

### Phase 3: Launch Scripts

**Outcome:** I can run `./run_trainer.sh` on the GPU machine and `./run_ui.sh --remote paul-cheddar`
on the laptop, and everything connects. One command per machine.

**Foundation:** Shell scripts that encode the deployment patterns. Future operators don't
need to remember port numbers or env vars.

**Files:**

| File | Change |
|------|--------|
| `run_trainer.sh` | **NEW** — starts trainer process. Prints Tailscale hostname and IP. |
| `run_ui.sh` | **NEW** — starts UI process. `--remote <hostname>` sets trainer URL. Default: local. |

**Tasks:**

1. Write `run_trainer.sh` — print IP addresses, start trainer
2. Write `run_ui.sh` — accept `--remote` flag, compute trainer URL, start UI with hot-reload
3. Verify: `./run_trainer.sh` on current machine, `./run_ui.sh` on current machine, dashboard works
4. Verify: `./run_ui.sh --remote paul-cheddar` connects to paul-cheddar (if reachable)

### Phase 4: End-to-End Verification

**Outcome:** I can run the full pipeline across machines: trainer on GPU, UI on laptop,
train a model, see live loss chart, save/load checkpoints.

**Verification steps (all must pass):**

1. Start trainer on current machine: `./run_trainer.sh`
2. Start UI on current machine: `./run_ui.sh`
3. Open browser — dashboard loads, health indicator green
4. Dashboard shows "Connected to http://localhost:8787"
5. Start UI with `--remote localhost` — still works (same machine, but via the flag)
6. Kill trainer — health indicator turns red within 3 seconds
7. Restart trainer — health indicator turns green within 3 seconds

**Cross-machine verification (requires access to paul-cheddar):**

8. SSH to paul-cheddar, git pull, `./run_trainer.sh`
9. On laptop: `./run_ui.sh --remote paul-cheddar`
10. Dashboard loads, health green, shows "Connected to http://paul-cheddar:8787"
11. Load MNIST via API, attach tasks, train 100 steps — loss chart updates live
12. Save checkpoint, load checkpoint — metrics revert

## Naming Conventions

- `AccConfig` — central config dataclass (not `Settings`, not `Env`)
- `--trainer-url` — CLI flag (not `--backend`, not `--api-url`)
- `--remote` — shell script flag for the common case (expands to `--trainer-url http://<host>:8787`)
- Health indicator uses `trainer_status` naming (not `backend_status`, not `api_status`)

## Phase Cleanup Notes

After M1.9:

- [ ] Should `_api()` in `ui/app.py` be made async? Currently blocks the event loop. Defer — works fine for single-user use.
- [ ] Should we add CORS headers to the trainer API? Not needed — browser talks to UI, not trainer directly.
- [ ] Should trainer API require authentication? Defer — Tailscale provides network-level auth.
- [ ] Should we add a `requirements.txt` or `pyproject.toml`? Yes, probably needed for the GPU machine setup.

## Full Outcome

After M1.9:

- Both processes work across Tailscale with one CLI flag each
- Dashboard shows live connection status to trainer
- Launch scripts encode deployment patterns
- Local dev still works with zero config (same defaults as before)
- Ready to actually train on GPU: clone repo on paul-cheddar, start trainer, connect from laptop

## How to Review

1. Read `acc/config.py` — the config dataclass
2. Run `./run_trainer.sh` — trainer starts, prints addresses
3. Run `./run_ui.sh` — UI starts, dashboard loads with health indicator
4. Kill trainer — health goes red. Restart — health goes green.
5. Run `./run_ui.sh --remote localhost` — same behavior via the flag
