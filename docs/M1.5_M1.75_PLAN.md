# M1.5 + M1.75 Plan — Model-Agnostic Forward Protocol + Factor-Slot Autoencoder

## Summary

M1 proved the full training loop works: model, tasks, train, eval, checkpoint. But the
Trainer is coupled to one model shape — it calls `get_pooled_latent()` and hands tasks a
flat tensor. That means every future model architecture requires either (a) faking the same
flat-tensor interface or (b) special-casing in the Trainer. Both are wrong.

M1.5 makes the pipeline model-agnostic. M1.75 proves it by running a completely different
architecture — the Factor-Slot Cross-Attention Autoencoder — through the same Trainer,
JobManager, CheckpointStore, and dashboard. Same task classes. Same training loop. Two
radically different models.

## Context & Motivation

The Factor-Slot architecture partitions the latent vector into named groups (position,
shape, color, free) where task probes read designated slices, not the whole vector.
Gradient isolation is structural: the position probe literally cannot touch the color dims.
The decoder reconstructs via cross-attention over factor embeddings, forcing spatial
specialization per factor.

Supporting this requires zero new task classes. A ClassificationTask that reads z[8:24] and
a ClassificationTask that reads all of z are the **same class with different config** — one
has `latent_slice=(8, 24)`, the other has `latent_slice=None`. This is the surgical win.

## Naming Conventions

- `ModelOutput` — enum for forward dict keys, lives in `acc/model_output.py`
- `FactorGroup` — dataclass for factor slice config, lives in `acc/factor_group.py`
- `FactorSlotAutoencoder` — the cross-attention model, lives in `acc/factor_slot_autoencoder.py`
- Factor layers — `acc/layers/factor_head.py`, `acc/layers/cross_attention.py`
- Synthetic data — `acc/generators/shapes.py`

## Phases

### Phase 1: M1.5 — Model-Agnostic Forward Protocol

**Outcome:** Any nn.Module whose `forward()` returns a `dict[str, Tensor]` keyed by
`ModelOutput` enum values can be trained by the Trainer. Tasks read what they need from the
dict. A task with `latent_slice=(start, end)` builds a probe on that slice instead of the
full latent. M1's test still passes identically — the Autoencoder returns the same data,
just in a dict instead of a tuple.

**Foundation:** `ModelOutput` enum is the contract between models and tasks. `latent_slice`
on Task replaces the need for per-architecture task subclasses. The Trainer is now a dumb
pipe — it calls `model.forward(images)`, gets a dict, passes it to the task. This pattern
supports N model architectures with zero Trainer changes.

**Verification:** `python -m acc.test_m1` passes (backward compat). `python -m acc.test_m1_5`
passes (slice targeting, dict protocol, out-of-bounds rejection).

**Files modified:**

| File | Change |
|------|--------|
| `acc/model_output.py` | **NEW** — `ModelOutput` enum (~15 lines) |
| `acc/autoencoder.py` | `forward()` returns `dict[str, Tensor]` instead of tuple. `get_pooled_latent()` preserved for convenience but delegates to `forward()` internally. |
| `acc/tasks/base.py` | Add `latent_slice: tuple[int, int] | None` field. `compute_loss` signature: `(model_output: dict, batch)`. `attach()` computes head dim from slice bounds. `_get_latent()` helper reads + slices from dict. |
| `acc/tasks/classification.py` | `compute_loss` reads via `_get_latent()`. `evaluate` calls `model.forward()` and reads latent from dict. |
| `acc/tasks/reconstruction.py` | `compute_loss` reads `RECONSTRUCTION` from dict — no more re-encode hack. `evaluate` same. |
| `acc/trainer.py` | Training loop: `model_output = self.autoencoder(batch[0])` then `task.compute_loss(model_output, batch)`. |
| `acc/test_m1.py` | Minimal updates to match new `forward()` return type. |
| `acc/test_m1_5.py` | **NEW** — verifies slice targeting, dict protocol, error on bad slice. |

**Tasks:**

1. Create `acc/model_output.py` with `ModelOutput` enum
2. Update `Autoencoder.forward()` to return `dict[str, Tensor]`
3. Update `Task` base class: add `latent_slice`, change `compute_loss` to take dict, add `_get_latent()` helper
4. Update `ClassificationTask` — use `_get_latent()`, update `evaluate` to use `forward()` dict
5. Update `ReconstructionTask` — read `RECONSTRUCTION` / `SPATIAL` from dict
6. Update `Trainer.train()` — call `forward()`, pass dict to task
7. Update `test_m1.py` for new signatures
8. Write `test_m1_5.py` — verify slice targeting + dict protocol
9. Run both tests, verify pass

### Phase 2: M1.75 — Factor-Slot Cross-Attention Autoencoder

**Outcome:** I can build a Factor-Slot autoencoder with designated factor groups and a
cross-attention decoder. I can attach tasks to specific latent slices. I can train it
through the exact same Trainer/JobManager/CheckpointStore pipeline that trains the simple
Autoencoder. Both models coexist. Same task classes work on both.

**Foundation:** `FactorGroup` dataclass defines latent layout. `FactorSlotAutoencoder` is
an independent `nn.Module` that returns a `ModelOutput` dict — including `FACTOR_SLICES`.
Cross-attention layers (`ResBlock`, `CrossAttentionBlock`, `CrossAttentionDecoder`) live in
`acc/layers/`. `RegressionTask` rounds out the task set. Synthetic shapes dataset provides
multi-factor labels for testing. The pipeline is proven model-agnostic: two architectures,
one Trainer.

**Verification:** `python -m acc.test_m1_75` builds a Factor-Slot model, attaches
classification + regression + reconstruction tasks with `latent_slice` config, trains 500
steps, evaluates, checkpoints, loads. Proves gradient isolation by cross-probing. All
previous tests still pass.

**Files:**

| File | Change |
|------|--------|
| `acc/factor_group.py` | **NEW** — `FactorGroup` dataclass |
| `acc/layers/res_block.py` | **NEW** — `ResBlock` (GroupNorm + SiLU + Conv) |
| `acc/layers/factor_head.py` | **NEW** — `FactorHead` (pool → project → slice) |
| `acc/layers/cross_attention.py` | **NEW** — `CrossAttentionBlock`, `FactorEmbedder` |
| `acc/factor_slot_autoencoder.py` | **NEW** — `FactorSlotAutoencoder` nn.Module |
| `acc/tasks/regression.py` | **NEW** — `RegressionTask` (MSE loss, MAE eval) |
| `acc/generators/shapes.py` | **NEW** — synthetic shapes with factor labels |
| `acc/trainer_api.py` | Update registry to include RegressionTask |
| `acc/layers/__init__.py` | Export new layers |
| `acc/test_m1_75.py` | **NEW** — full verification |

**Tasks:**

1. Create `acc/factor_group.py` with `FactorGroup` dataclass
2. Create `acc/layers/res_block.py` — ResBlock
3. Create `acc/layers/factor_head.py` — FactorHead
4. Create `acc/layers/cross_attention.py` — CrossAttentionBlock, FactorEmbedder
5. Create `acc/factor_slot_autoencoder.py` — full model returning ModelOutput dict
6. Create `acc/tasks/regression.py` — RegressionTask
7. Create `acc/generators/shapes.py` — synthetic shapes with position/scale/shape labels
8. Update `acc/trainer_api.py` — registry includes new types
9. Write `acc/test_m1_75.py` — full loop: factor model + sliced tasks + train + eval + checkpoint
10. Run ALL tests: `test_m1`, `test_m1_5`, `test_m1_75`

## Phase Cleanup Notes

After M1.75:

- [ ] Does `Autoencoder` (simple) need to populate `FACTOR_SLICES`? No — tasks check for the key.
- [ ] Should `get_pooled_latent()` be deprecated in favor of `forward()`? Defer — it's still convenient for quick scripting.
- [ ] Is the synthetic shapes generator rich enough for real factor-slot experiments? Probably not — but it proves the pipeline. Real generators come in M3.
- [ ] Should the UI dashboard show factor groups and attention maps? Defer to M2+ — M1.75 is code-first verification.

## Full Outcome

After M1.75:

- The pipeline is proven model-agnostic: two architecturally different models, same Trainer.
- Task classes are reusable across models via `latent_slice` config — no per-model subclasses.
- The `ModelOutput` enum is the contract. New models just populate the dict.
- Factor-Slot autoencoder trains with cross-attention decoding and slice-targeted probes.
- All three test scripts pass: `test_m1`, `test_m1_5`, `test_m1_75`.

## Directory Structure (After M1.75)

```
acc/
├── __init__.py
├── model_output.py              # ModelOutput enum
├── autoencoder.py               # Simple autoencoder (M1, updated for dict forward)
├── factor_group.py              # FactorGroup dataclass
├── factor_slot_autoencoder.py   # Factor-Slot cross-attention autoencoder
├── dataset.py                   # AccDataset, load_mnist
├── trainer.py                   # Trainer (model-agnostic)
├── jobs.py                      # JobManager
├── checkpoints.py               # CheckpointStore
├── trainer_api.py               # HTTP API
├── trainer_main.py              # Trainer process entry
├── ui_main.py                   # UI process entry
├── test_m1.py                   # M1 verification
├── test_m1_5.py                 # M1.5 verification (dict protocol + slicing)
├── test_m1_75.py                # M1.75 verification (factor-slot model)
├── tasks/
│   ├── __init__.py
│   ├── base.py                  # Task base (with latent_slice)
│   ├── classification.py        # ClassificationTask
│   ├── reconstruction.py        # ReconstructionTask
│   └── regression.py            # RegressionTask (NEW)
├── layers/
│   ├── __init__.py
│   ├── conv_block.py            # ConvBlock, ConvTransposeBlock
│   ├── res_block.py             # ResBlock (NEW)
│   ├── factor_head.py           # FactorHead (NEW)
│   └── cross_attention.py       # CrossAttentionBlock, FactorEmbedder (NEW)
├── generators/
│   ├── __init__.py
│   └── shapes.py                # Synthetic shapes generator (NEW)
├── ui/
│   ├── __init__.py
│   ├── app.py
│   └── static/
├── eval/
│   └── __init__.py
├── data/
└── checkpoints/
```

## How to Review

1. Read `acc/model_output.py` — the 15-line enum that defines the contract
2. Read `acc/autoencoder.py` `forward()` — verify it returns the dict
3. Read `acc/tasks/base.py` — verify `latent_slice` + `_get_latent()` helper
4. Run `python -m acc.test_m1` — M1 still passes
5. Run `python -m acc.test_m1_5` — dict protocol + slicing works
6. Read `acc/factor_slot_autoencoder.py` — verify it returns same dict shape
7. Run `python -m acc.test_m1_75` — factor-slot trains through same pipeline
