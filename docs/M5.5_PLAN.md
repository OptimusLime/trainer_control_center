# M5.5 Plan: Loss Diagnosis Dashboard

## Summary

M5.5 makes the dashboard scream when something is wrong. Every training step emits per-task loss with health classification. The dashboard displays live loss stats with red/yellow/green flagging, summary statistics per-task (mean, trend, final), and persistent loss summaries stored in checkpoints. Job history shows at-a-glance health. You can never again train for 5000 steps with a broken reconstruction loss and not know about it.

## Context & Motivation

The user trained a full two-branch experiment (10,000 total steps). The reconstruction is visibly broken — the model produces garbage. But the dashboard showed **zero indication** of this. The loss chart renders lines with no context. A KL of 12.0 and a recon loss of 0.4 both display identically. Job history shows final losses as flat text with no color. There is no concept of "healthy" vs "unhealthy" loss values. The checkpoint `metrics` field is always empty.

**This is the exact purpose of the dashboard.** The dashboard must be the user's eyes, and right now it is blind.

### What's broken today:

1. **No health indicators on live losses.** The loss chart and log show raw numbers with no color or context.
2. **No summary statistics.** No mean/min/max/trend per task during or after training. Just a raw scrolling log.
3. **No loss data in checkpoints.** `Checkpoint.metrics` is always `{}`. When the trainer restarts, all job history is gone.
4. **Job history shows final losses as flat monochrome text.** No red flags for bad values.
5. **No way to compare loss curves across jobs.** Can't overlay two jobs' losses on one chart.

## Naming Conventions

- `LossHealth` — enum with `HEALTHY`, `WARNING`, `CRITICAL` (in `acc/loss_health.py`)
- `LossSummary` — dataclass with mean/min/max/final/trend/health per task (in `acc/loss_health.py`)
- `step_info["health"]` — health classification string added to each step's data
- `Checkpoint.metrics["loss_summary"]` — persisted loss summary dict
- API: `GET /jobs/{job_id}/loss_summary` — per-task summary with health
- UI: `partial_loss_summary()` — live-updating summary panel with color coding
- CSS: `.loss-healthy`, `.loss-warning`, `.loss-critical` — color classes

## Phases

### Phase 1: Loss Health Classification + Enriched Step Info

**Outcome:** Every training step emits a health classification alongside the loss value. The `step_info` dict gains a `"health"` field (`"healthy"`, `"warning"`, `"critical"`). The SSE stream and loss log show colored values in real time.

**Foundation:** `LossHealth` enum and `classify_loss()` function in `acc/loss_health.py`. Reusable health classification based on task type and loss value ranges. Same thresholds used everywhere (trainer, UI, job history).

**Verification:**
1. Start training with the current model. SSE stream includes `"health"` field in each step.
2. Loss log entries are colored red/yellow/green based on health.
3. `classify_loss("recon", 0.4)` returns `CRITICAL`; `classify_loss("recon", 0.02)` returns `HEALTHY`.

Tasks:
1. Create `acc/loss_health.py` with:
   - `LossHealth(str, Enum)` — `HEALTHY = "healthy"`, `WARNING = "warning"`, `CRITICAL = "critical"`
   - `classify_loss(task_name: str, task_type: str, loss_value: float) -> LossHealth` — rule-based classification
   - Thresholds per task type: `ReconstructionTask` (L1 on [0,1] images: healthy < 0.05, warning < 0.15, critical >= 0.15), `KLDivergenceTask` (healthy < 5.0, warning < 15.0, critical >= 15.0), `ClassificationTask` (CE: healthy < 0.5, warning < 2.0, critical >= 2.0), `RegressionTask` (MSE: healthy < 0.1, warning < 0.5, critical >= 0.5)
2. Modify `Trainer.train()` — after computing `loss.item()`, call `classify_loss()` and add `"health"` + `"task_type"` to `step_info`
3. Modify `_chart_js()` in UI — color the loss log entries by health field (green/yellow/red)
4. Modify `addLossPoint()` JS — accept health param, color the log line accordingly

### Phase 2: Per-Task Loss Summary Statistics

**Outcome:** During and after training, the dashboard shows a summary table: per-task mean loss, final loss, min/max, trend (improving/worsening/flat), and overall health. This table updates live during training and is the primary "am I fucked?" indicator.

**Foundation:** `LossSummary` dataclass and `compute_loss_summary(losses: list[dict]) -> dict[str, LossSummary]` function. This summary computation is reusable for any list of step_info dicts — live training, historical jobs, or checkpoint comparison.

**Verification:**
1. Train 500 steps. The loss summary panel shows per-task stats (mean, final, trend) with color.
2. After training completes, summary persists and is visible.
3. `compute_loss_summary()` works on any `list[dict]` of step_infos — tested programmatically.

Tasks:
1. Add to `acc/loss_health.py`:
   - `LossSummary` dataclass: `task_name: str`, `task_type: str`, `mean: float`, `final: float`, `min: float`, `max: float`, `trend: str` (improving/worsening/flat), `health: LossHealth`, `n_steps: int`
   - `compute_loss_summary(losses: list[dict]) -> dict[str, LossSummary]` — groups by task_name, computes stats, classifies health from final value, computes trend from first-half vs second-half mean
2. Add `GET /jobs/{job_id}/loss_summary` API endpoint — returns per-task LossSummary as JSON
3. Add `GET /jobs/current/loss_summary` API endpoint — for live training
4. Add `partial_loss_summary()` UI panel — colored table with per-task rows, auto-refreshes during training
5. Wire into dashboard layout: new panel between training chart and job history

### Phase 3: Loss Persistence in Checkpoints + Job History Enhancement

**Outcome:** When a checkpoint is saved (manually or by recipe), the loss summary from the most recent job is stored in `Checkpoint.metrics["loss_summary"]`. Job history items show colored final losses. Checkpoint tree shows health indicators.

**Foundation:** `CheckpointStore.save()` gains optional `loss_summary` param that populates `Checkpoint.metrics`. The summary is persisted in the `.pt` file and survives trainer restart.

**Verification:**
1. Train 500 steps, save checkpoint. Reload checkpoint metadata — `metrics["loss_summary"]` contains per-task stats.
2. Job history panel shows colored loss values (red for bad, green for good).
3. Checkpoint tree nodes show health indicator (green dot / red dot).

Tasks:
1. Modify `CheckpointStore.save()` — accept optional `loss_summary: dict` param, store in `checkpoint.metrics["loss_summary"]`
2. Modify trainer API `save_checkpoint` and recipe `ctx.save_checkpoint` — pass loss summary from most recent job
3. Modify `partial_jobs_history()` UI — color final losses by health classification
4. Modify `partial_checkpoints_tree()` UI — show health indicator dot per checkpoint (from metrics)
5. Modify `partial_training()` UI — show current loss health prominently (banner at top of training panel)

## Phase Cleanup Notes

- `_tensor_to_base64` still lives in `trainer_api.py` — defer to M6
- Loss summary computation lives in `acc/loss_health.py` which is a pure library module — good
- Consider whether `LossHealth` thresholds should be configurable per-task instance rather than per-task-type — defer, hardcoded thresholds are fine for now

### Cleanup Decision (at phase end)

- **Do now:** Ensure all existing `.metric-good`/`.metric-mid`/`.metric-bad` CSS classes are consistent with the new `.loss-healthy`/`.loss-warning`/`.loss-critical` classes
- **Defer:** Per-task-instance configurable thresholds
- **Drop:** N/A

## Full Outcome Across All Phases

After M5.5, the user can:
1. See real-time health classification on every training step (green/yellow/red in the loss log)
2. See per-task summary statistics (mean, final, min, max, trend) with color coding during and after training
3. Load any checkpoint and see its loss summary (persisted in checkpoint metadata)
4. See at-a-glance health in job history (colored final losses)
5. See health indicators on checkpoint tree nodes
6. **Never again train for thousands of steps without the dashboard alerting them that losses are unhealthy**

## Directory Structure

```
acc/
├── loss_health.py          # NEW — LossHealth enum, LossSummary, classify_loss(), compute_loss_summary()
├── trainer.py              # MODIFIED — step_info gains "health" and "task_type" fields
├── trainer_api.py          # MODIFIED — new loss_summary endpoints, save_checkpoint passes summary
├── checkpoints.py          # MODIFIED — save() accepts loss_summary
├── ui/
│   └── app.py              # MODIFIED — colored loss log, loss summary panel, colored job history, checkpoint health dots
```

## How to Review

1. `python -m acc.test_m5_5` — all tests pass
2. Open dashboard, train 500 steps — loss log shows colored entries, summary panel shows per-task stats with green/yellow/red
3. Save checkpoint — reload page, checkpoint tree shows health indicator
4. Check job history — final losses are colored by health
5. Intentionally train with broken model — dashboard shows red everywhere
