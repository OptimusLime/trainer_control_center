# M1.95: Recipes + Checkpoint Tree + Experiment Runner

## Status: Planning

---

## What This Is

A recipe is a program that operates on the checkpoint tree. It creates models, loads datasets, attaches tasks, forks checkpoints, trains, evaluates, and forks again. The checkpoint tree IS the experiment. The recipe describes operations on the tree.

**Functionality:** I can select a recipe from the dashboard, click Run, and watch it build a checkpoint tree — forking, configuring, training, and evaluating each branch. I can see each branch's progress, compare eval results across branches, and later reproduce any node by replaying the recipe operations up to that point.

**Foundation:** `Recipe` base class with tree operations (`fork`, `configure`, `train`, `eval`). `RecipeRunner` that executes recipe steps and reports progress. `RecipeRegistry` with hot-reload (same `importlib.reload` pattern as tasks). `CheckpointTree` visualization in the dashboard sidebar. This is the abstraction that makes experiments repeatable and checkpoint graphs reproducible.

---

## The Experiment

We are testing one hypothesis: **does synthetic task curriculum improve disentanglement in a factor-slot VAE?**

Two runs. Same architecture. Same factor layout. Same model weights at fork point. The ONLY difference is whether synthetic tasks are attached.

```
Recipe: MNIST Factor Experiment
│
├─ Common setup: Build FactorSlotAutoencoder, load MNIST 32×32
│  Factor groups: digit(0:4), thickness(4:7), slant(7:10), free(10:16)
│  Save checkpoint "root"
│
├─ Fork "root" → "mnist_only"
│  ├─ Attach: ReconTask + per-factor KL + DigitClassification(MNIST labels)
│  ├─ NO synthetic data. Thickness/slant slots learn from recon+KL only.
│  ├─ Train 5000 steps → checkpoint "mnist_only_5k"
│  └─ Eval → traversals, sort-by-activation, attention maps
│
└─ Fork "root" → "with_curriculum"
   ├─ Attach: ReconTask + per-factor KL + DigitClassification(MNIST labels)
   │         + ThicknessRegression(synthetic) + SlantRegression(synthetic)
   ├─ Generate synthetic ThicknessDataset + SlantDataset
   ├─ Train 5000 steps → checkpoint "curriculum_5k"
   └─ Eval → traversals, sort-by-activation, attention maps

Compare: mnist_only_5k vs curriculum_5k
```

**If curriculum_5k has cleaner thickness/slant traversals than mnist_only_5k, the synthetic tasks worked.** Same model, same code, same fork point. The only variable is the synthetic curriculum.

---

## Why Recipes Need Forks

Fork is not a future feature. Fork is how experiments work. Without fork, you can't branch from a common root, which means you can't isolate variables, which means you can't do science. Both branches start from identical model weights at "root". Any difference in the result came from the training, not from initialization luck.

---

## Architecture

### Recipe Base Class

```python
class Recipe:
    """A program that operates on the checkpoint tree."""

    name: str                    # Human-readable name
    description: str             # What this recipe tests

    def run(self, ctx: RecipeContext) -> None:
        """Execute the recipe. ctx provides tree operations."""
        raise NotImplementedError
```

### RecipeContext — the operations a recipe can perform

```python
class RecipeContext:
    """Operations available to a recipe. Wraps TrainerAPI state."""

    def create_model(self, builder: Callable) -> None
        """Set the current model. builder() returns an nn.Module."""

    def load_dataset(self, name: str, loader: Callable) -> AccDataset
        """Load or generate a dataset. loader() returns AccDataset."""

    def attach_task(self, task: Task) -> None
        """Attach a task to the current model."""

    def detach_all_tasks(self) -> None
        """Remove all tasks. Used before reconfiguring a fork."""

    def save_checkpoint(self, tag: str) -> str
        """Save current state. Returns checkpoint_id."""

    def fork(self, checkpoint_id: str, tag: str) -> str
        """Fork from a checkpoint, load that state, return new checkpoint_id.
        This is: copy .pt file, load it, set as current. The recipe
        continues from this forked state."""

    def train(self, steps: int, lr: float = 1e-3) -> JobInfo
        """Train for N steps. Blocks until complete. Returns job info."""

    def evaluate(self) -> dict[str, dict[str, float]]
        """Run eval on all tasks. Returns {task_name: {metric: value}}."""

    def log(self, message: str) -> None
        """Log a message to the recipe progress stream."""

    @property
    def current_checkpoint_id(self) -> str | None

    @property
    def phase(self) -> str
        """Current phase name for progress display."""

    @phase.setter
    def phase(self, name: str) -> None
```

### RecipeRunner

Wraps `TrainerAPI` state and executes a recipe in a background thread. Reports progress via the same SSE mechanism as training jobs.

```python
class RecipeRunner:
    """Executes a recipe and reports progress."""

    def start(self, recipe: Recipe, api: TrainerAPI) -> RecipeJob
    def stop(self) -> None
    def current(self) -> RecipeJob | None
```

`RecipeJob` tracks recipe phases:

```python
@dataclass
class RecipeJob:
    id: str
    recipe_name: str
    state: str           # "running", "completed", "stopped", "failed"
    current_phase: str   # e.g., "Fork root → mnist_only"
    phases_completed: list[str]
    checkpoints_created: list[str]  # checkpoint IDs created by this recipe
    started_at: datetime
    error: str | None = None
```

### RecipeRegistry

Same pattern as task discovery, but for `acc/recipes/`:

- File watcher on `acc/recipes/` directory
- On `.py` file change: `importlib.reload`, scan for `Recipe` subclasses, update registry
- Syntax errors caught and reported, don't crash the trainer
- Dashboard recipe picker populated from registry

### Trainer API Endpoints (new)

```
GET  /recipes                    → list available recipes
GET  /recipes/{name}             → recipe details
POST /recipes/{name}/run         → start a recipe
POST /recipes/stop               → stop running recipe
GET  /recipes/current            → current recipe job status
GET  /recipes/current/stream     → SSE stream of recipe progress

GET  /checkpoints/tree           → full tree structure (nodes + edges)
POST /checkpoints/{id}/fork      → fork a checkpoint (also used by recipes)
```

---

## The Built-in Recipe

File: `acc/recipes/mnist_factor_experiment.py`

```python
class MNISTFactorExperiment(Recipe):
    name = "mnist_factor_experiment"
    description = "Two-branch comparison: factor-slot MNIST-only vs factor-slot + synthetic curriculum"

    def run(self, ctx):
        # === Common setup ===
        ctx.phase = "Load MNIST 32x32"
        mnist = ctx.load_dataset("mnist_32", lambda: load_mnist(image_size=32))

        ctx.phase = "Build FactorSlotAutoencoder"
        factor_groups = [
            FactorGroup("digit", 0, 4),
            FactorGroup("thickness", 4, 7),
            FactorGroup("slant", 7, 10),
            FactorGroup("free", 10, 16),
        ]
        ctx.create_model(lambda: FactorSlotAutoencoder(
            in_channels=1, factor_groups=factor_groups, image_size=32
        ))

        ctx.phase = "Save root checkpoint"
        root_id = ctx.save_checkpoint("experiment_root")

        # === Branch 1: MNIST only (no synthetic tasks) ===
        ctx.phase = "Fork → mnist_only"
        ctx.fork(root_id, "mnist_only")
        ctx.attach_task(ReconstructionTask("recon", mnist))
        ctx.attach_task(KLDivergenceTask("kl", mnist, weight=1.0))
        ctx.attach_task(ClassificationTask(
            "digit_classify", mnist, latent_slice=(0, 4)
        ))
        # Thickness/slant slots get gradient ONLY from recon + KL

        ctx.phase = "Train mnist_only (5000 steps)"
        ctx.train(steps=5000, lr=1e-3)
        ctx.save_checkpoint("mnist_only_5k")

        ctx.phase = "Eval mnist_only"
        ctx.evaluate()

        # === Branch 2: MNIST + synthetic curriculum ===
        ctx.phase = "Fork → with_curriculum"
        ctx.fork(root_id, "with_curriculum")
        ctx.attach_task(ReconstructionTask("recon", mnist))
        ctx.attach_task(KLDivergenceTask("kl", mnist, weight=1.0))
        ctx.attach_task(ClassificationTask(
            "digit_classify", mnist, latent_slice=(0, 4)
        ))

        ctx.phase = "Generate synthetic datasets"
        thickness_ds = ctx.load_dataset("thickness_synth",
            lambda: generate_thickness(n=5000, image_size=32))
        slant_ds = ctx.load_dataset("slant_synth",
            lambda: generate_slant(n=5000, image_size=32))

        ctx.attach_task(RegressionTask(
            "thickness", thickness_ds, output_dim=1, latent_slice=(4, 7)
        ))
        ctx.attach_task(RegressionTask(
            "slant", slant_ds, output_dim=1, latent_slice=(7, 10)
        ))

        ctx.phase = "Train with_curriculum (5000 steps)"
        ctx.train(steps=5000, lr=1e-3)
        ctx.save_checkpoint("curriculum_5k")

        ctx.phase = "Eval with_curriculum"
        ctx.evaluate()

        ctx.phase = "Complete — compare mnist_only_5k vs curriculum_5k"
```

---

## New Components to Build

### 1. FactorSlotAutoencoder updates for VAE (mu/logvar + reparameterization)

The existing `FactorSlotAutoencoder` factor heads output deterministic latent slices. For KL regularization to work, each factor head needs to output `(mu, logvar)` and reparameterize. Changes to:

- `acc/layers/factor_head.py` — `FactorHead` outputs `2 * factor_dim` (mu + logvar), reparameterizes, returns `(z_sample, mu, logvar)`
- `acc/factor_slot_autoencoder.py` — collects per-factor mu/logvar, concatenates into full mu/logvar tensors, adds `ModelOutput.MU` and `ModelOutput.LOGVAR` to output dict
- `acc/model_output.py` — add `MU` and `LOGVAR` keys

The `FactorSlotAutoencoder` remains the SAME architecture. We're adding the VAE reparameterization path so KL works. No architectural changes to encoder backbone, cross-attention decoder, or factor group layout.

### 2. KL Divergence Task

File: `acc/tasks/kl_divergence.py`

```python
class KLDivergenceTask(Task):
    """KL divergence regularization toward N(0,1).

    Reads ModelOutput.MU and ModelOutput.LOGVAR.
    Weight controls beta: weight=1.0 is standard VAE, weight>1.0 is beta-VAE.
    Supports per-factor KL via latent_slice.
    """
    # check_compatible: model forward must return MU and LOGVAR
    # compute_loss: -0.5 * mean(1 + logvar - mu^2 - exp(logvar))
    # evaluate: returns {"kl": average_kl_per_dim}
```

Per-factor KL is just `KLDivergenceTask` with `latent_slice=(start, end)`. Tighter KL on designated factors (crisp traversals), looser on free dims (let them be messy). Tunable weight per instance from the dashboard.

### 3. Synthetic Generators

File: `acc/generators/thickness.py`
```python
def generate_thickness(n: int, image_size: int = 32) -> AccDataset:
    """Lines/curves at controlled stroke widths on 32x32 grayscale.
    White strokes on black background (like MNIST).
    target = stroke_width (float, normalized 0-1)."""
```

File: `acc/generators/slant.py`
```python
def generate_slant(n: int, image_size: int = 32) -> AccDataset:
    """Strokes at controlled angles on 32x32 grayscale.
    White strokes on black background (like MNIST).
    target = angle (float, normalized 0-1 mapping -45 to +45 degrees)."""
```

### 4. Recipe Infrastructure

Files:
- `acc/recipes/__init__.py`
- `acc/recipes/base.py` — `Recipe`, `RecipeContext`, `RecipeJob`
- `acc/recipes/runner.py` — `RecipeRunner`
- `acc/recipes/registry.py` — `RecipeRegistry` (file watcher + importlib.reload)
- `acc/recipes/mnist_factor_experiment.py` — the two-branch experiment

### 5. Checkpoint Tree Visualization

Existing `CheckpointStore` already has `parent_id` tracking and `fork()`. What's needed:
- `GET /checkpoints/tree` endpoint returns nodes + edges for the dashboard
- Dashboard sidebar renders the tree (not a flat list)
- Current checkpoint highlighted
- Fork button on each node

### 6. Dashboard Recipe UI

- Recipe picker (dropdown populated from `RecipeRegistry`)
- Run button → starts `RecipeRunner`
- Phase progress display (current phase name + completed phases)
- Auto-updating checkpoint tree as recipe creates forks

---

## Eval Visualizations (dashboard panels)

These render for whatever checkpoint is currently loaded. Compare by loading each branch's checkpoint and looking.

### Latent Traversals

For each factor group (digit, thickness, slant, free):
- Pick 5 seed digits from MNIST test set
- Encode each → z
- Hold all OTHER factor dims fixed
- Vary this factor's dims from -3 to +3 in 9 steps
- Decode each → grid of 5 rows × 9 columns

**What to look for:** mnist_only_5k thickness traversal is mushy/entangled. curriculum_5k thickness traversal cleanly varies stroke width without changing digit identity.

### Sort by Factor Activation

For each factor group:
- Encode all MNIST test digits
- Compute mean activation of this factor slice
- Sort all digits by this value
- Display 20 lowest and 20 highest as thumbnail grids

**What to look for:** curriculum_5k thickness sort shows thin→thick. mnist_only_5k thickness sort shows... whatever it learned (probably nothing clean).

### Cross-Attention Maps

For a few MNIST images, visualize which spatial positions attend to which factor tokens in the decoder. One heatmap per factor, overlaid on the input.

**What to look for:** Does the thickness token attend to stroke bodies? Does the digit token attend to discriminative features? Compare between branches.

---

## Implementation Phases

### Phase 1: VAE updates (mu/logvar + reparameterization + KL task)

**Build:**
- Update `FactorHead` to output mu/logvar and reparameterize
- Update `FactorSlotAutoencoder` to collect per-factor mu/logvar into `ModelOutput`
- Add `ModelOutput.MU` and `ModelOutput.LOGVAR`
- Implement `KLDivergenceTask`

**Verify:**
```python
# Build FactorSlotAutoencoder with factor groups
# Forward pass → ModelOutput contains MU, LOGVAR, LATENT, RECONSTRUCTION
# Attach KLDivergenceTask, compute_loss returns scalar
# Train 200 steps with ReconTask + KLTask → both losses decrease
```

### Phase 2: Synthetic generators

**Build:**
- `acc/generators/thickness.py` — `generate_thickness()`
- `acc/generators/slant.py` — `generate_slant()`

**Verify:**
```python
# Generate 100 thickness images, verify shape [100, 1, 32, 32], labels in [0, 1]
# Generate 100 slant images, same checks
# Visual inspection: white strokes on black, controlled variation
```

### Phase 3: Recipe infrastructure + checkpoint tree API

**Build:**
- `Recipe` base class, `RecipeContext`, `RecipeJob`, `RecipeRunner`
- `RecipeRegistry` (discover from `acc/recipes/`)
- Trainer API endpoints: `GET /recipes`, `POST /recipes/{name}/run`, `GET /recipes/current`, `GET /recipes/current/stream`
- `GET /checkpoints/tree` returns tree structure
- `POST /checkpoints/{id}/fork` exposed properly
- Wire `RecipeContext` methods to `TrainerAPI` state (create_model, load_dataset, attach_task, fork, train, evaluate)

**Verify:**
```python
# Run a trivial test recipe from Python that:
#   1. Creates a model
#   2. Saves "root" checkpoint
#   3. Forks to "branch_a", trains 100 steps, saves
#   4. Forks root to "branch_b", trains 100 steps, saves
# Verify /checkpoints/tree returns: root → branch_a, root → branch_b
```

### Phase 4: The built-in recipe

**Build:**
- `acc/recipes/mnist_factor_experiment.py` — the full two-branch experiment
- Recipe hot-reload (file watcher on `acc/recipes/`)

**Verify:**
```python
# python -m acc.test_m1_95
# Runs MNISTFactorExperiment programmatically
# Verifies:
#   - FactorSlotAutoencoder created with correct factor groups
#   - MNIST loaded at 32×32
#   - "experiment_root" checkpoint saved
#   - Fork to "mnist_only": recon + KL + digit classify, train 5000, eval
#   - Fork to "with_curriculum": recon + KL + digit classify + thickness + slant, train 5000, eval
#   - Checkpoint tree: root → mnist_only_5k, root → curriculum_5k
#   - Both branches have eval metrics
```

### Phase 5: Dashboard UI

**Build:**
- Recipe picker + run button
- Phase progress indicator
- Checkpoint tree visualization (replace flat list)
- Fork button on checkpoint nodes
- Eval visualization panels: traversal grids, sort-by-activation, attention maps

**Verify:**
- Select "mnist_factor_experiment" → click Run → watch phases progress
- Checkpoint tree shows two branches from root
- Load mnist_only_5k → see traversals and sorted grids
- Load curriculum_5k → see traversals and sorted grids
- Compare with your eyes

---

## Verification Script: `python -m acc.test_m1_95`

```python
"""M1.95 verification — recipes + checkpoint tree + experiment runner."""

def test_factor_slot_vae():
    """FactorSlotAutoencoder with reparameterization returns MU, LOGVAR, LATENT."""

def test_kl_task():
    """KLDivergenceTask computes KL from MU/LOGVAR, supports per-factor slice."""

def test_synthetic_generators():
    """Thickness and slant generators produce valid AccDatasets."""

def test_recipe_infrastructure():
    """Recipe base class, context, runner, registry all work."""

def test_mnist_factor_experiment():
    """Two-branch recipe: fork from root, each branch trains and evals."""
    # 1. Recipe creates "experiment_root" checkpoint
    # 2. Forks to "mnist_only", trains with recon+KL+digit, saves "mnist_only_5k"
    # 3. Forks to "with_curriculum", trains with recon+KL+digit+thickness+slant, saves "curriculum_5k"
    # 4. Checkpoint tree: root → mnist_only_5k, root → curriculum_5k
    # 5. Both branches have eval metrics

def test_checkpoint_tree_api():
    """GET /checkpoints/tree returns correct tree after recipe run."""

def test_recipe_hot_reload():
    """Write a .py recipe file, it appears in registry within 2 seconds."""
```

---

## Impact on M2-M6

| Milestone | Before | After M1.95 |
|-----------|--------|-------------|
| **M2** | Hot-reload tasks + registry | Narrowed: task hot-reload only (registry pattern already built in M1.95 for recipes). Wire same file watcher to `acc/tasks/`. |
| **M3** | Synthetic generators + hot-reload | Narrowed: generator hot-reload + dashboard generator UI (thickness/slant generators already built in M1.95). |
| **M4** | Checkpoint tree + forking | **Absorbed into M1.95.** Tree structure, fork operation, tree visualization all ship here. |
| **M5** | UFR evaluation | Augmented: traversal grids, sort-by-activation, attention maps from the experiment become eval visualizations. UFR score computation on top. |
| **M6** | Model expansion | Unchanged. |

---

## Open Design Decisions

1. **32×32 input size:** Current `FactorSlotAutoencoder` defaults to `image_size=64` with 3 stride-2 backbone convs → 8×8 spatial. At 32×32 that gives 4×4 spatial, which is tight for cross-attention. Options: (a) reduce to 2 backbone convs → 8×8 spatial at 32×32, or (b) keep 3 convs and accept 4×4 spatial. Recipe model builder decides.

2. **Per-factor KL weights:** The `KLDivergenceTask` takes a `weight` param for the overall beta. Per-factor KL means creating separate `KLDivergenceTask` instances with different `latent_slice` and `weight` values. Example: tight KL (weight=4.0) on digit/thickness/slant, loose KL (weight=0.1) on free dims. This is just task configuration, no special code needed.

3. **Recipe blocking vs async:** `ctx.train(steps=5000)` blocks. `RecipeRunner` runs the recipe in a background thread. Progress reporting via SSE, same mechanism as `JobManager`.
